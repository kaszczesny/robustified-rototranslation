\chapter{Analysis and improvements of \textit{Rebvo} algorithm}
\label{cha:intro2}

This chapter contains thorough analysis of proposed algorithm. Firstly, algorithm is carefully explained, step by step, using various examples. Symbols used through the chapter are explained in Section~\ref{sec:struct}. Much attention is paid to edge detector, because it also had to be implemented, as available ready-made edge detection methods did not meet algorithm requirements. Chapter is concluded with miscellaneous remarks and experiment results. Figures used throughout the chapter were generated using input images from TUM \cite{tum} and KITTI \cite{kitti} datasets.

Note: considerations, improvements and modifications of the original algorithm are \textbf{emphasized}.

median filter; regularize before and after Kalman


scale from dimensions of known real-world objects, such as road signs or license plates



\section{Notation (Keyline structure) and {\tt main} loop}
\label{sec:struct}

Pixels that contain subpixel edge positions are called Keylines and, after edge extraction, are stored as an array of structures defined in Table \ref{tab:keyline}. When $(n+1)$-th frame of video input is being processed, only $n$-th and $(n+1)$-th Keyline arrays are available; $n-1$-th is discarded, as it is no longer needed. This is presented in Fig.~\ref{fig:flowchart_main}. Fields of $n$-th Keyline array will be denoted with subscript $_{p}$ (for \textit{previous}) and $_{t}$ (for \textit{transformed}). Subscript $_{c}$ (\textit{current}) will be associated with $(n+1)$-th frame.

\begin{figure}[hp]
	\begin{footnotesize}

	\centering\begin{tikzpicture}[node distance = 2cm, auto]
	% Place nodes
	
	\node [cloud] (start) {start};
	\node [block, below of=start] (b0) {{\tt n := 1}};
	\node [block, below of=b0] (b05) {{\tt parameters\_initialization()}};
	\node [block, below of=b05] (b1) {{\tt KLs\textsubscript{p} := edge\_detection(n) }};
	
	\node [block, below of=b1] (b2) {{\tt n++}};
	\node [block, below of=b2] (b3) {{\tt KLs\textsubscript{c} := edge\_detection(n) }};
	
	\node [block, below of=b3] (b4) {{\tt edge\_tracking()}};
	
	\node [decision, below of=b4] (b5) {minimization successful?};
	
	\node [block, below of=b5, node distance=8.5em] (b6) {{\tt mapping()}};
	
	\node [decision, below of=b6, node distance=8.5em] (b7) {\\*$card(KLs_{new}.m_d) > 500$?};
	
	\node [block, right of=b5, text width=10em, xshift=10em] (b8) {{\tt KLs\textsubscript{p} := KLs\textsubscript{c}}};
	% Draw edges
	
	\path [line] (start) -- (b0);
	\path [line] (b0) -- (b05);
	\path [line] (b05) -- (b1);
	\path [line] (b1) -- (b2);
	\path [line] (b2) -- (b3);
	\path [line] (b3) -- (b4);
	\path [line] (b4) -- (b5);
	\path [line] (b5.west) -| node [near start] {no} ([xshift=-1cm] b05.west)
	                       |- (b05.west);
	\path [line] (b5.south) -- node {yes} (b6);
	\path [line] (b6) -- (b7);
	\path [line] (b7.west) -| node [near start] {no} ([xshift=-1cm] b05.west)
					   	   |- (b05.west);
	\path [line] (b7.east) -| node [near start] {yes} (b8.south);
	\path [line] (b8.north) |- (b2.east);
	\end{tikzpicture}
	
	\caption{Simplified flowchart of the algorithm. $KLs_{x}$ are arrays of Keyline structures}
	\label{fig:flowchart_main}
		\end{footnotesize}
\end{figure}

\begin{table}[ht]
	\centering
	
	\begin{threeparttable}
		\caption{Keyline structure}
		\label{tab:keyline}
		
		\begin{tabularx}{1.0\textwidth}{C{0.25} C{0.75}}
			\toprule
			\thead{Structure field} & \thead{Description} \\
			\midrule
			$q = [q_{x},\ q_{y}]$ & subpixel position in image \\
			$h = [h_{x},\ h_{y}]$ & normalized\tnote{a} $q$: $h = q - [c_{x},\ c_{y}]$ \\
			$\rho$, $\sigma_{\rho}$ & estimated inverse depth: $\frac{1}{Z}$, and its variance \\
			$\rho_0$, $\sigma_{\rho_{0}}$ & inverse depth predicted by Kalman filter and its variance \\
			$\vec{g}$ & edge gradient obtained from DoG \\
			$p_{id}, n_{id}$ & index of previous and next Keyline in an edge \\
			$m_f$ & index of next frame Keyline obtained during forward matching \\
			$m_d$ & index of next frame Keyline obtained during directed matching \\
			$k$ & number of consecutive frames that this Keyline has appeared on \\
			%$h_0$ &  \\
			$\vec{g_{0}}$ & gradient of matched Keyline from previous frame \\
			\bottomrule
		\end{tabularx}
		
		\begin{tablenotes}
			\footnotesize
			\item[a] Usage of normalized coordinates is more robust \cite{hartley1997defense}. In the implementation, $h$ is often also temporarily divided by $f$, so that calculations on homogeneous coordinates are more stable.
		\end{tablenotes}
		
	\end{threeparttable}
\end{table}

% ---

\section{Edge extraction}

Primal step of algorithm is subpixel edge extraction. Keyline structures are populated using extracted data. Keylines that are estimated to lie on same edge are joined.

\subsection{Edge detection algorithm choice}
While many edge detection algorithms could be used in this step, authors of \textit{Rebvo} have chosen DoG (Difference of Gaussians), because it provides \cite{jose2015realtime}:
\begin{enumerate}
	\item repetivity -- an edge is detected similarly throughout consecutive frames,
	\item precision -- edge positions are accurate,
	\item low time \& memory complexity.
\end{enumerate}

Another advantage of DoG, unmentioned by Tarrio and Pedre, is that \textbf{edge gradient can be obtained directly} from normal vector of the fitted local plane. In this thesis DoG was used as well. Most vital property was subpixel precision, otherwise Canny detector \cite{canny} would be employed. In principle, subpixel edge detection precision is possible, because as long as Whittaker–Nyquist–Kotelnikov–Shannon theorem assumptions are satisfied, the true continuous pixel intensity function can be reconstructed from discrete image values.

\subsection{Data preprocessing}

First of all, images are converted to grayscale. Color information is not used in the algorithm.

Before performing any edge detection technique, one critical issue has to be addressed. If input images were rectified\footnote{If images were \textit{not} rectified, they should be.} in such a way that extrapolation beyond original borders had been needed, artificial edges will be present in every frame (because extrapolation procedure usually assumes 0 pixel intensity beyond the image). Gradient of these edges is almost always strong, meaning that they tend to pass all tests and to be identified as valid keylines, thus generating false positives. During later minimization step, they greatly distort the procedure, making it behave as if the 3D space was curved. Example of such border is depicted in Fig.~\ref{fig:rectifcy_border}.

\begin{figure}[ht]
	\centering\includegraphics[width=0.75\linewidth, trim={1cm 1cm 1cm 1cm},clip]{img/figures/rectified_edge.png}
	\caption{ Input image, zoomed-in near left border. Individual pixels produced by rectification can be observed on the left }
	\label{fig:rectifcy_border}
\end{figure}

In case of DoG, these \textbf{artificial edges need to be removed} before Gaussian blurring -- otherwise they would spread out, making it tricky to reject them later. In tested datasets, artificial borders' thickness did not exceed 1~pixel, so simply 1-pixel wide frame of outermost pixels was always discarded. In general case, \textbf{width of the frame can be figured out from distortion model}, instead of being set manually. However, for highly distorted images, a rectangular frame would also discard many valid pixels -- either in corners (barrel distortions), or near middle of borders (pincushion distortions).

\subsection{Difference of Gaussians strength tests}

After necessary preprocessing, edge detection can be started. Generally edge detection works by finding positions in image where pixel intensity changes most rapidly. Basic idea of DoG is to approximate image second derivative (the laplacian) \cite{szeliski} \cite{jain1995machine}. The zero-crossing of laplacian corresponds to such positions. Example based on real data, reduced to one dimension for clarity, is presented on Fig.~\ref{fig:slice}. Other approaches to edge detection involve curve fitting \cite{fabijanska} \cite{devernay1995non} \cite{wei2010two}.

\begin{figure}[ht]
	\centering\includegraphics[width=0.75\linewidth, trim={1.5cm 1.5cm 1.5cm 1.0cm},clip]{img/figures/subpixelslice.png}
	\caption{ DoG zero-crossing subpixel edge detection. Discussed method was applied to an image, then one row of pixels containing very apparent edge was extracted. Red $\times$ denotes subpixel edge position }
	\label{fig:slice}
\end{figure}

In order to filter out noise, image should be first smoothed with a Gaussian filter. These operations can be combined into one operator -- Laplacian of Gaussian -- which in turn can be approximated with difference of two images smoothed with Gaussian filters using two different sigmas\footnote{LoG is best approximated by DoG when $\frac{\sigma_{1}}{\sigma_{2}} = \sqrt{2}$ \cite{sift}.} Edge detection results for initial sigma values were satisfactory in performed tests, therefore other sigma values (or automated sigma adjustment schemes) were not tested.

Exemplary Difference of Gaussians image is depicted in Fig.~\ref{fig:dog}. \textit{Implementation insight}:~while individual smoothed images can be computed using {\tt uint8} as underlying data type (for speed), the difference has to be computed using signed data type. Otherwise obtained function will not cross zero!

\begin{figure}[ht]
	\centering\includegraphics[width=0.75\linewidth, trim={1.25cm 1.25cm 1.25cm 1cm},clip]{img/figures/dog.png}
	\caption{ Difference of Gaussians applied to an image. Near-zero values indicate feasible edge candidates }
	\label{fig:dog}
\end{figure}

Another parameter is window size -- it defines pixel neighborhood that will take part in later calculations of edge position. For window size $w$, $(2w+1)^2$ immediate neighbors will be considered, including the center pixel itself. For sake of this thesis, value $w = 2$ was used.

\subsubsection{First test}
\label{edge_first}

Edge detection procedure is performed for every pixel that lies at least $w$ pixels from image border, so that neighborhood does not need to be extrapolated. In order to speed up computations by early identifying non-edges, first derivative of pixel intensity is calculated by applying two Sobel operators (derivatives along $x$ and $y$ axes) and taking norm of the result. This norm is then compared with a threshold. 

Tarrio and Pedre use hysteresis to determine threshold value after every frame. On the one hand, presented algorithm operates offline, so abundance of keylines is not an issue. On the other, hysteresis parameters still need to be adjusted for given dataset. Invalid parameters can alter overall algorithm results significantly, so from algorithm analysis point of view, the fewer of them, the better. \textbf{Thus instead simpler solution was tested out}.

Image is be partitioned into $a^2$ rectangular chunks, for relatively small $a$, e.g. 7. Then for each chunk number of pixels that would pass the first derivative test for given constant threshold is counted. If this number is relatively low, threshold is locally multiplied by a constant ${b_{1} < 1}$, e.g. $\frac{1}{3}$. However, if the number is relatively large, then threshold is locally multiplied by ${b_{2} > 1}$, e.g. $\frac{5}{3}$. Local threshold array can finally be smoothed with Gaussian filter to avoid discontinuities.

Proposed procedure also has parameters, but there are few advantages:
\begin{itemize}
	\item parameters are much simpler to interpret,
	\item effectively only one parameter is crucial (the constant threshold) and rest of procedure serves as a refinement,
	\item operates without delay, which is vital especially if keyline number is too low.
\end{itemize}

Fig.~\ref{fig:bucket} illustrates that additional pixels were considered further for being edges. However, edges added by this method are not very stable and tend to be nonetheless untraceable in later frames (they do not exhibit the \textit{repetivity} property).

TODO: figure

\subsubsection{Second test}
\label{edge_second}

Neighborhood of pixels that have passed the first test is checked for ``sign balance``. Number of positive and negative DoG values has to be comparable within a defined percentage, e.g. 20\%. Example results of the test for 2 different pixels are depicted in Fig.~\ref{fig:plusminus}. It was observed that pixels very rarely fail this particular test.

\begin{figure}[ht]
	\centering\includegraphics[width=0.75\linewidth, trim={1.5cm 2.5cm 1.5cm 2.5cm},clip]{img/plusminus.png}
	\caption{ Example of the DoG sign balance test. Negative values are denoted as black pixels, postive -- as white. Green color marks neighborhood of pixel that has passed the test and is, possibly, an edge. Pixel marked with red was rejected }
	\label{fig:plusminus}
\end{figure}

\subsubsection{Third test}
\label{edge_third}

Then DoG values are approximated by a plane using linear regression (Eq.~\ref{eq:regress} is solved for~$\theta$). Zero-crossing of the plane can be determined algebraically, resulting in Eq.~\ref{eq:zerocross}. For more resilience, an additional test is performed. If inequality~\ref{eq:test3} is not satisfied, then equation system~\ref{eq:regress} \textbf{is considered to be badly conditioned} and this pixel is rejected. This was not considered in \cite{jose2015realtime}. The purpose of this test is to avoid divide-by-zero errors in fourth test (Sec.~\ref{edge_fourth}). All pixels that do not pass the third test would still be filtered out later -- they also do not pass the fifth test~(Sec.~\ref{edge_fith}).

\begin{equation}
\bm{A}\theta = \delta
\label{eq:regress}
\end{equation}
where:
\begin{eqwhere}[2cm]
	\item[$\bm{A}$] 3x$(2w+1)$ matrix: $[X\ Y\ 1]$,
	\item[$X$] vector of $x$ coordinates of pixel centroids in neighborhood, assuming that central pixel's centroid is located at $x = 0$, ($(2w+1)$-tuple),
	\item[$Y$] vector of $y$ coordinates of pixel centroids in neighborhood, assuming that central pixel's centroid is located at $y = 0$, ($(2w+1)$-tuple),
	\item[$\delta$] DoG values corresponding to $X$ and $Y$ ($(2w+1)$-tuple),
	\item[$\theta$] parameters defining the approximated plane: $z = \theta_{x}x + \theta_{y}y + \theta_{z}$.
\end{eqwhere}



\begin{equation}
\begin{bmatrix}
x_s \\
y_s 
\end{bmatrix} = 
\begin{bmatrix}
\frac{-\theta_{x} \theta{y}}{\theta_x^2 + \theta_y^2} \\
\frac{-\theta_{y} \theta{y}}{\theta_x^2 + \theta_y^2}
\end{bmatrix}
\label{eq:zerocross}
\end{equation}
where:
\begin{eqwhere}[2cm]
	\item[$k_s$] estimated $k$-coordinate of the subpixel edge position (0 is the pixel center).
\end{eqwhere}

\begin{equation}
\theta_{x}^2 + \theta_{y}^2 > 10^{-6}
\label{eq:test3}
\end{equation}

\subsubsection{Fourth test}
\label{edge_fourth}

Obtained zero-crossing marks the subpixel position of the edge. Inequality~\ref{eq:subpix_inside} tests whether it lies within the pixel itself. If not -- edge is not detected. After this test, most of Keyline candidates form 1-pixel wide edges.

todo quiver

\begin{equation}
max(|x_s|, |y_s|) < 0.5
\label{eq:subpix_inside}
\end{equation}

\subsubsection{Fifth test}
\label{edge_fith}

Normal vector of fitted plane is defined by Eq.~\ref{eq:planenorm}. Vector $\vec{r}$ can projected onto $z=0$ plane, creating third derivative vector: the edge gradient $\vec{g}$. If $||\vec{g}||$ is small, then $r_{z}$ component must have dominated $\vec{r}$, meaning that fitted plane was almost parallel to the $z=0$ plane. In turn this implies that edge was not sharp. Therefore, as a final test, norm of $\vec{g}$ is tested -- it must exceed a~threshold for edge to be finally detected.

\begin{equation}
\vec{r} = [\theta_{x},\ \ \theta_{y},\ \ -1]
\label{eq:planenorm}
\end{equation}
where:
\begin{eqwhere}[2cm]
	\item[$\vec{r}$] normal vector of plane $z = \theta_{x}x + \theta_{y}y + \theta_{z}$.
\end{eqwhere}

\subsubsection{Edge detection tests summary}

Influence of all aforementioned tests to final result is depicted for an exemplary frame in Fig.~\ref{fig:edgeprob}. Colored regions do resemble to some degree a probability distribution -- probability that given pixel contains an edge. Thus initially use of fuzzy logic was considered. Ultimately, as already mentioned, detected edges were deemed to be acceptable, so this idea was not pursued. Such approach would also require more processing power, making it less suitable for mobile devices.

\begin{figure}[ht]
	\centering\includegraphics[width=1.0\linewidth, trim={1.5cm 2.5cm 1.5cm 2.5cm},clip]{img/figures/edge_prob.png}
	\caption{ Edge detection tests results. Pixel colors: dark blue -- neighborhood outside image; blue, light blue (not present), cyan (not present), green and orange -- rejected by tests 1 to 5, respectively; red -- reserved for debug purposes (not present); dark red -- final Keylines}
	\label{fig:edgeprob}
\end{figure}


\subsection{Depth initialization}

After Keyline has been successfully identified on image, a data structure described in Section~\ref{sec:struct} is populated with obtained data. This step is different for the very first processed frame, because its Keylines must have some initial depth values. During first tests, a constant $\rho = 1$ was used. Then for some time explicit Kinect depth map was used, as it was available in the TUM dataset. Finally, after other ares of algorithm have been improved, it was observed that in most cases quick convergence (5-10 frames) could be achieved by \textbf{initializing depth with random values}, even with images scaled down by 80\% (to 128 by 96 pixels). Convergence depends on speed of camera during these first frames. Overall, this is much better result than in \cite{jose2015realtime}, where 2-5 seconds needed for depth convergence were reported. Noise was generated using normal distribution with parameters chosen for each dataset: $N(2,0.5)$ for TUM and $N(10,3)$ for KITTI.


Some \textbf{thought was given to more elaborate initialization schemes}. Feature point correspondences could be used in order to determine scene geometry in a quicker and more robust way, while rest of algorithm would still operate on edges. Feature point descriptors, such as SIFT \cite{sift}, or even simpler corner detectors \cite{harris} \cite{shi1994good}, are well suited for this task.

Their main disadvantage is their complexity, especially in case of more robust ones, like SIFT. They need to be somehow matched -- this becomes unfeasible for real-time applications once number feature points is too large \cite{szczesny}. This in turn means that many parameters would have to be carefully chosen so that a consumer-grade mobile system would not be flooded by excessive features and remain responsive. Finally, feature point descriptors encode larger pixel neighborhood than edges, but they lack structural information, meaning that match is likely produce more outliers, that need to be dealt with (filtered).



todo figure



\subsection{Keyline joining}

After obtaining individual Keylines, they are joined together to form connected edges.
For each Keyline, its neighbors are searched among 3 out of 8 bordering pixels. Search is performed in direction perpendicular to $\vec{g}$, an example is depicted in Fig.~\ref{fig:edgejoin}.

\begin{figure}[ht]
	\centering\includegraphics[width=0.75\linewidth, trim={1.25cm 1.25cm 1.25cm 1cm},clip]{img/edgejoin.png}
	\caption{ Edge joining principle. White pixels denote Keylines, black pixels~\==~non-Keylines, green x -- currently processed pixel, green arrow -- $\vec{g}$, red arrow -- $\vec{g}$ rotated by $\frac{\pi}{2}$ clockwise , red rectangle -- edge joining candidates }
	\label{fig:edgejoin}
\end{figure}


Joined Keylines are used for pruning. First of all, edges consisting of 3 pixels or less are discarded, as they are unlikely to be good features to track. Secondly, outermost Keylines of every joined edge are likewise removed -- out of all Keylines in that edge, they are most likely to be affected by noise.

In later steps, the algorithm considers only individual Keylines, not joined edges (as mentioned in Section~\ref{sec:rebvo_outline}). Information about neighbors is used after edge detection only once -- in Regularization step, where $\rho$ and $\sigma_{\rho}$
are averaged over Keyline and its 2 immediate neighbors.

Initially \textbf{more edge joining strategies were considered}: morphological operations on Keylines, loop avoidance and edge segmentation (into parts with similar gradient). However, once it has been understood that edge joining takes such small part in algorithm flow and that by-pixel approach is preferable \cite{harris}, this direction of research was abandoned.


%--

\section{Edge tracking}

Goal of edge tracking is to find a 3D transformation (translation and rotation) that best describes transition between two consecutive frames. Previous frame Keylines are projected to 3D space, where they are rotated and translated; then resulting points are back-projected to image plane (transformations are elaborated upon in Section~\ref{sec:warp}). Reprojected Keylines are matched against Keylines of the next frame -- see Sections~\ref{sec:aux}~and~\ref{sec:minim_match}. Minimization of the residual is performed using Levenberg-Marquardt algorithm. Similarly to~\cite{jose2015realtime}, implementation is based on \cite{madsen2004methods}.

\subsection{Warping function}
\label{sec:warp}

2D Keyline $\longleftrightarrow$ 3D point transformations are defined by Eq.~\ref{eq:gamma}~and Eq.~\ref{eq:gammainv}. They can be derived from \ref{eq:camera}, assuming $f_x = f_y = f$ and $s = 0$. These assumptions were valid for tested datasets. It should be noted, however, that in case of smartphone cameras, \textbf{full pinhole model should be considered}, complicating these formulas. Once points have been projected using $\gamma$, they can be rotated and translated in 3D space using Eq.\ref{eq:rototr}.


\begin{equation}
\gamma(h_{p}, \rho_{p}) = \left [\frac{h_{p_x}}{f\rho_{p}},\ \ \frac{h_{p_y}}{f\rho_{p}},\ \ \frac{1}{\rho_{p}}\right ]: \Re^2 \times \Re \rightarrow \Re^3
\label{eq:gamma}
\end{equation}
where:
\begin{eqwhere}[2cm]
	\item[$\gamma$] projection function.
\end{eqwhere}

\begin{equation}
\gamma^{-1}(Q) = \left [ \left [ \frac{fQ_x}{Q_z},\ \ \frac{fQ_y}{Q_z} \right],\ \ \frac{1}{Q_{z}}\right ]: \Re^3 \rightarrow  \Re^2 \times \Re
\label{eq:gammainv}
\end{equation}
where:
\begin{eqwhere}[2cm]
	\item[$\gamma^{-1}$] back-projection function,
	\item[$Q$] 3D world point (3-tuple).
\end{eqwhere}

\begin{equation}
\psi(\vec{v}, \vec{\omega}, Q) = \mathfrak{exp} \left ( \left[ \vec{\omega} \right ]_{s} \right ) Q + \vec{v}
\label{eq:rototr}
\end{equation}
where:
\begin{eqwhere}[2cm]
	\item[$\psi$] warping function,
	\item[$\vec{v}$] camera translation (3-tuple),
	\item[$\vec{\omega}$] camera azimuth rotation in axis-angle notation\footnote{Axis-angle vector $\vec{\omega}$ describes right-hand rotation by $||\vec{\omega}||$ radians about $O\omega$ axis.} (3-tuple),
	\item[$ \mathfrak{exp}$] matrix exponentiation function which can be approximated using Euler-Rodrigues formula \cite{opencv}.
\end{eqwhere}


\subsection{Auxiliary image}
\label{sec:aux}

Auxiliary image is a lookup table that speeds up minimization. It is a matrix of size equal to the image. An entry in the matrix is nonzero if distance from its coordinates to closest Keyline's $q$ is lower than a search radius $r_{search}$, usually equal to 5 pixels. Then such entry contains a reference to the found closest Keyline.

During minimization, Keylines from previous frame are projected to 3D space using depth information ($\rho$), rototranslated, and then projected back to image plane. Positions obtained from the back-projection are compared against the auxiliary image of next frame to check if such position corresponds to a~Keyline (and to which).

Auxiliary image creation can be perceived as widening of an edge by ``spanning`` its pixels along $\vec{g}$. A depiction is presented in Fig.~todo. Because pixel positions are discrete, \textbf{increments of this spanning should be lower than 1 px}. Otherwise, when $\vec{g}$ is not perpendicular to pixel grid, edge widening process will occasionally skip over some pixels. This leads to discontinuities (gaps) in auxiliary image (see Fig.~todo), which can later bias the minimization by producing false local minima.

Some gaps are unavoidable, however, because $\vec{g}$ is available only in places where there is a~Keyline. Even with subpixel position precision, there still is only at most one Keyline per pixel. If edge has any curvature, then at some radius auxiliary image will feature spikes. They are visible in top left corner of Fig.~todo.

todo: figure



\subsection{Keyline matching criteria}
\label{sec:minim_match}

During each iteration of minimization, reprojected old Keylines and new Keylines are pre-matched, using auxiliary image. In order to determine validity and score of this match, following criteria are tested. Result of applying them on a frame is depicted in Fig.~\ref{fig:minim_why}.

\begin{enumerate}
	\item History -- if $k_{p}$ is lower than some constant threshold $k_{thresh}$ (e.g. 2), then this Keyline is considered to be too uncertain (its history is too short) and it does not take part in minimization at all. This test is performed only after $k_{thresh}$ frames have been wholly processed by the algorithm in order to allow system to initialize.
	\item Depth uncertainty -- Keylines where $\sigma_{\rho_{p}}$ exceeds its 95. percentile are rejected.
	\item Gradient similarity -- score based on angle between $\vec{g_p}$ and $\vec{g_c}$, as well as on their magnitudes, can not exceed a threshold.
\end{enumerate}

\begin{figure}[ht]
	\centering\includegraphics[width=0.85\linewidth, trim={1.5cm 1.5cm 1.5cm 1cm},clip]{img/figures/minim_why.png}
	\caption{ Minimization tests results. Pixel colors: dark blue -- not Keyline; blue~\==~ $\sigma_{\rho}$ too high; cyan -- too short history; teal -- 2D reprojection lies outside the image; orange -- no corresponding Keyline in next frame's auxiliary image; red -- gradients dissimilar; dark red -- successful matching }
	\label{fig:minim_why}
\end{figure}

\subsection{Energy minimization}
\label{sec:energyminim}

Each minimization iteration executes following steps:
\begin{itemize}
	\item Using previously estimated Jacobian matrix $J$ and other information from previous iteration, propose new parameter vector in the 6-dimensional parameter space $\left[ \vec{v},\ \ \vec{\omega} \right]$.
	\item For each Keyline in previous frame:
	\begin{itemize}
		\item Perform reprojection: $h_{t} = \left( \gamma^{-1} \circ \psi \circ \gamma \right) \left( h_p, \rho_{p} \right)$.
		\item Find closest $h_{c}$ that conforms to similarity criteria.
		\item Calculate the residual.
	\end{itemize}
	\item Calculate overall iteration score.
\end{itemize}

Minimized function takes form defined in Eq.~\ref{eq:minim}. Distance between matched points is casted onto edge gradient, because \textit{``location information is only present in direction [of the gradient]``}  \cite{jose2015realtime}. The matching function $M$ is not differentiable, making it impossible to calculate Jacobian of $\zeta$ analytically. Therefore Jacobians were calculated using the same formulas as in Tarrio and Pedre's implementation.

\begin{equation}
E = \zeta \left( \vec{v},\ \ \vec{\omega} \right) = \sum_{i}^{p} \frac{w_i}{\sigma_{\rho_{i}}^2} \left[ \underset{h_t, h_c}{M} \left( \left( h_t \right) \bullet \vec{g_c} \right) \right] ^2
\label{eq:minim}
\end{equation}
where:
\begin{eqwhere}[2cm]
	\item[$w_i$] weight (square of the Huber norm \cite{huber1964robust}),
	\item[$h_t$] transformed $h_p$, $h_{t} = \gamma^{-1}\left(\ \  \psi \left( \vec{v},\ \vec{\omega},\ \gamma \left( h_p, \rho_{p} \right) \right)\ \  \right)$,
	\item[$M$] matching function, $\underset{h_t}{M}(\bullet) = \begin{cases} 
	\bullet, & \exists h_c : h_t \textrm{ matches } h_c \\
	r_{search}, & otherwise
	\end{cases}$
\end{eqwhere}

todo wykres histogramu, wykres jak spada score, wykres że kierunek
we get residual (DResidual) \& weighted residual (fm) projected in direction of gradient

Increment of the input vector is dictated by the Levenberg–Marquardt update equation \cite{press2007numerical}. One way of determining it is to use SVD on a 6x6 matrix $\Lambda$, as it is performed by Tarrio and Pedre. However, $\Lambda$ is always a positive definite \cite{madsen2004methods}, meaning that \textbf{Cholesky factorization can be used}, instead of slower, but less constrictive SVD. However, size of the decomposed matrix is quite small, so computational gain is negligible.

This was tested in GNU Octave, using OpenCV \cite{opencv} linear algebra routines, compiled to MEX files. Random values were chosen to populate variables in the LM equation and then it was solved $10^6$ times, first using SVD, then Cholesky factorization. Average running times per iteration have been collected in Tab.~\ref{tab:chol}.

\begin{table}[ht]
	\centering
	
	\begin{threeparttable}
		\caption{SVD and Cholesky running time comparison}
		\label{tab:chol}
		
		\begin{tabularx}{0.6\textwidth}{C{0.5} C{0.5}}
			\toprule
			\thead{Method} & \thead{Average time} \\
			\midrule
			SVD & $29 \mu s$ \\
			Cholesky & $24 \mu s$ \\
			\bottomrule
		\end{tabularx}
		
	\end{threeparttable}
\end{table}


After optimal transformation parameters have been found, uncertainty of the result is estimated as covariance matrix in form $\left( J^T J \right) ^ {-1}$. \textbf{If $J^T J$ is ill-conditioned, then it is assumed that minimization has failed.} For example, this can happen if no matches at all were found. Then all residuals will be equal and $\zeta$ will be completely flat locally.

Moreover, after last minimization iteration, the $m_{pf}$ field of previous Keylines is populated. For every Keyline that was successfully matched to a current Keyline, reference to this current Keyline is saved in the said field.

\subsection{Initial conditions}

Levenberg-Marquardt algorithm is prone to falling to local minima. Repetitive patterns, such as fences, are especially challenging (see Fig.~2 in \cite{parra2008robust}). Generally, global minimization of arbitrary function is an intricate problem with no infallible solution. There has been proposed a~plethora of heuristic methods: evolutionary algorithms, simulated annealing, physical simulations (Momentum), etc. However, they are unsuitable for real-time applications \cite{szeliski}.

In case of LM, crucial issue is choice of initial condition vectors. Following \cite{jose2015realtime}, two initial conditions were tested, and calculations proceeded using better one. These vectors are: $\vec{0}$ and rototranslation of the previous frame. If linear and angular instantaneous velocities of the camera do not change rapidly, it is reasonable to assume that previous and current velocities are not far apart in the parameter space.

One exception is the special case when too few Keylines are matched and system decides to reinitialize. This usually happens when number of detected Keylines also drops -- for instance in the ``\textit{fr2\_360\_kidnap}`` sequence. In the context of algorithm results postprocessing (i.e. fitting the estimated camera trajectory to ground truth data), it is better to assume no prior knowledge and overwrite the previous parameter vector with zeros.

During some tests, the prior rototranslation vector for the first frame in sequence was obtained from ground truth data in hope of accelerating algorithm convergence (and to assess whether it could retain good information that was fed to it). However, usually ground truth data uses other reference frame, as position of the laser positioning system is different from position of the camera itself.

%--

\section{Mapping}

For obtained information to be useful in the next iteration of the algorithm, it needs to be preserved. Since this is not a full SLAM system, data needs to be updated and passed on to next array of Keylines. This is achieved by the mapping step: Keylines from previous iteration are matched to Keylines that are currently being processed. Data that is passed between matched Keylines on contains:

\begin{itemize}
	\item estimated $\rho$ and $\sigma_{\rho}$, that need to be refined with the Kalman filter,
	\item history ($k$),
	\item index of matching Keyline from the previous frame.
\end{itemize}

Initial matching is also performed during minimization. As it was noted in Section~\ref{sec:rebvo_outline}, it is quite coarse, however. In addition, there is another reason for such elaborate match searching scheme. After system has been initialized (i.e. after $k_{thresh}$ frames have been processed), history test will become active in the minimizer. Only Keylines already having some matching history will have possibility of being matched again and propagated further. On the other hand, ``new`` Keylines with no prior history won't have opportunity of gaining it, because they will be always skipped in the very first minimizer test and not matched.

Finally, the mapping step contains depth information inter-frame processing procedures.

\subsection{Forward matching}


Since calculated transformation between the two frames is supposed to be the optimal one, after applying it to 3D positions of previous Keylines and casting them into the image plane, their expected positions on the next frame will be obtained. This was already done at the minimization step; said Keylines were then paired with their corresponding match using property $m_pf$ (see conclusion of Section~\ref{sec:energyminim}). Forward matching simply uses this information and copies appropriate field values from previous Keylines to their new forward matched counterparts. If later a better match is found, already copied data will be simply overwritten. \textit{Forward} refers to the fact that minimizer creates initial matches from previous frame to current.

\subsection{Directed matching}

pokazac wyfilotwane outliery

%directed match
%
% for each KL:
%
%   backrotate KL and cast it onto old edge map mask
%
%   using this position and backrotated velocity, obtain halfline in which this KL could have been moved (a halfline, because we only know that its rho is positive) - estimated pixel displacement
%
%   estimate uncertainty using projected position and Rvel
%
%   caonstrain displacement search radius \& define starting point
%
% todo: check if perpedicular displacement if really common
Considering that there might have been some outliers that affected the quality calculated transformation and that potentially valid matches with no history need a chance to pick it up, it is essential to extend the list of matches to include Keylines that have not been back-projected perfectly.

So as to make finding candidates for matching easier, current Keyline array is reverse-rotated\footnote{Rotated using rotation opposite to rotation $\vec{\omega}$ that was obtained during minimization.} and casted onto the old edge map. This decreases the distance between corresponding points and reduces the problem to a pure translatory problem. Translation $\vec{v}$ calculated during minimization, along with the reverse-rotated $h_c$, defines for each current Keyline a line. (todo details)


Because $\rho_c > 0$, this line is reducted to a halfline.  Halfline starting at projected point two-dimensional position and going in the direction of camera 2D velocity is defined - possible matches should lie on that halfline, because of the assumption that rho of any Keyline should be positive. To further constrain search area, maximum and minimum pixel displacement are estimated. Keylines that lie on such defined segment are possible candidates for a match.

% search from starting point alternating between one direction and the other
%
% check if this pixel there is a KL to match to
%
% perform 3 tests to filter outliers + image (eg. 3 consecutive images where an outlier disappears):
%
%  gradient angle similarity
%
%  gradient modulus similarity
%
%  motion consistency
Starting at the most possible point, searching is performing by alternating between one direction and the other. If currently checked pixel of the edge mask contains a Keyline, it is considered a match, however that match still needs to be evaluated. Gradient on those matching pixels needs to be similar enough in the means of gradient angle difference and gradient modulus. Also, potential match needs to conform to the motion model.

%rownanie, costam costam, ogarnialismy

This provides additional outlier rejection - any match that is not compatible with model is rejected. This means that any Keyline which motion is different from the most of them will most likely belong to a object moving independently from the scene, as mentioned in /rozdzial costam/ 

After those two matching steps, number of matches between two frames has to be greater than previously defined threshold (500 Keylines). If its less than that, algorithm is reseted, as there is not enough Keylines with established history to base future transformation calculations on.

\subsection{Regularization}

% optional regularization, performed twice
%
% main assumption is that KLs neighboring on an edge ale located near each other in 3D, so their depth should be similiar
%

As edges of real-life objects are seldom very jagged, difference in depth between adjoining Keylines needs to be smoothed out, assuming that neighboring pixels belong to objects located near each other in three-dimensional space. This is done by taking weighted mean of rho and s\_rho of directly joined Keylines. Two iterations of regularization are performed. To be regularized, a Keyline needs to have two neighbors (this means excluding ends of edges).

% for each KL:
%
%  check if 2 neighbors pass test:
%
%   if depths outweight uncertainties (probabilistic uncertainty)
%
%   if angle between gradients is below threshold
%
%  if tests are passed, rho and s\_rho are smoothened taking weighted mean of its value and its neighbors

% image: show that assumption isn't ALWAYS true
Having in mind previous assumption, that might not always be the case. Objects might lie on the scene in such a way, that their edges as seen by the camera might intersect and form a~continuous curve. This is why additional test on neighboring pixels are needed. First, they need to lie on approximately the same distance from the camera. Because depth is estimated, uncertainty on said estimation is calculated in /gdzie to bylo?/ step. If difference between estimated depth for two neighbors outweights sum of uncertainties, it is assumed that they belong to different objects. This test is purely probabilistic. Second, angle between gradients of neighboring Keylines is checked. Again, if its greater than previously defined threshold, regularization is not performed.

\subsection{Depth reestimation}

 kalman fiter

 ...

 inverse depth is constrained between certain min and max

\subsection{Scale correction}


 optional estimate rescaling

 according to Tarrio et al, EKF is biased in rho estimation, so a global "shrinking factor" can be applied to depths and uncertainties.

todo 2x depth

% ----------------------







