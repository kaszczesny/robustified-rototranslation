\chapter{Analysis and improvements of \textit{Rebvo} algorithm}
\label{cha:intro2}

This chapter contains thorough analysis of proposed algorithm. Firstly, changes to the original algorithm are listed. Then algorithm is carefully explained, step by step, using various examples. Much attention is paid to edge detector, because available ready-made edge detection methods did not meet algorithm requirements. Chapter is concluded with miscellaneous remarks and experiment results. Figures used throughout the chapter were generated using input images from TUM \cite{tum} and KITTI \cite{kitti} datasets.

Note: considerations, improvements and modifications of the original algorithm are emphasized by \underline{underlining}.

\section{Changes to the algorithm}
\label{sec:changes}

TODO:

edge detection bucketing

median filter; regularize before and after Kalman

rho initialization (noise instead of ones)

small numerical improvements in LM


\section{Other considered changes}
\label{sec:rejected}

TODO:

scale from dimensions of known real-world objects, such as road signs or license plates

initialization from classical feature matching algorithm

keyline joining improvements: morphological operations, Y junctions

usage of corners

automatic DoG sigma choice


\section{Notation (Keyline structure) and {\tt main} loop}
\label{sec:struct}

Pixels that contain subpixel edge positions are called Keylines and, after edge extraction, are stored as an array of structures defined in Table \ref{tab:keyline}. When $n+1$-th frame of video input is being processed, only $n$-th and $(n+1)$-th Keyline arrays are available; $n-1$-th is discarded, as it is no longer needed. This is presented in Fig.~\ref{fig:flowchart_main}.

\begin{figure}[hp]
	\begin{footnotesize}

	\centering\begin{tikzpicture}[node distance = 2cm, auto]
	% Place nodes
	
	\node [cloud] (start) {start};
	\node [block, below of=start] (b0) {{\tt n := 1}};
	\node [block, below of=b0] (b05) {{\tt parameters\_initialization()}};
	\node [block, below of=b05] (b1) {{\tt KLs\textsubscript{old} := edge\_detection(n) }};
	
	\node [block, below of=b1] (b2) {{\tt n++}};
	\node [block, below of=b2] (b3) {{\tt KLs\textsubscript{new} := edge\_detection(n) }};
	
	\node [block, below of=b3] (b4) {{\tt edge\_tracking()}};
	
	\node [decision, below of=b4] (b5) {minimization successful?};
	
	\node [block, below of=b5, node distance=8.5em] (b6) {{\tt mapping()}};
	
	\node [decision, below of=b6, node distance=8.5em] (b7) {\\*$card(KLs_{new}.m_d) > 500$?};
	
	\node [block, right of=b5, text width=10em, xshift=10em] (b8) {{\tt KLs\textsubscript{old} := KLs\textsubscript{new}}};
	% Draw edges
	
	\path [line] (start) -- (b0);
	\path [line] (b0) -- (b05);
	\path [line] (b05) -- (b1);
	\path [line] (b1) -- (b2);
	\path [line] (b2) -- (b3);
	\path [line] (b3) -- (b4);
	\path [line] (b4) -- (b5);
	\path [line] (b5.west) -| node [near start] {no} ([xshift=-1cm] b05.west)
	                       |- (b05.west);
	\path [line] (b5.south) -- node {yes} (b6);
	\path [line] (b6) -- (b7);
	\path [line] (b7.west) -| node [near start] {no} ([xshift=-1cm] b05.west)
					   	   |- (b05.west);
	\path [line] (b7.east) -| node [near start] {yes} (b8.south);
	\path [line] (b8.north) |- (b2.east);
	\end{tikzpicture}
	
	\caption{Simplified flowchart of the algorithm. $KLs_{x}$ are arrays of Keyline structures}
	\label{fig:flowchart_main}
		\end{footnotesize}
\end{figure}

\begin{table}[ht]
	\centering
	
	\begin{threeparttable}
		\caption{Keyline structure}
		\label{tab:keyline}
		
		\begin{tabularx}{1.0\textwidth}{C{0.25} C{0.75}}
			\toprule
			\thead{Structure field\tnote{a}} & \thead{Description} \\
			\midrule
			$q = [q_{x},\ q_{y}]$ & subpixel position in image \\
			$h = [h_{x},\ h_{y}]$ & normalized $q$: $h = \frac{q - [c_{x}\ c_{y}]}{z_f}$ \\
			$\rho$, $\sigma_{\rho}$ & estimated inverse depth: $\frac{1}{Z}$, and its variance \\
			$\rho_0$, $\sigma_{\rho_{0}}$ & inverse depth predicted by Kalman filter and its variance \\
			$\vec{g}$ & edge gradient obtained from DoG \\
			$p_{id}, n_{id}$ & index of previous and next Keyline in an edge \\
			$m_f$ & index of next frame Keyline obtained during forward matching \\
			$m_d$ & index of next frame Keyline obtained during directed matching \\
			$k$ & number of consecutive frames that this Keyline has appeared on \\
			%$h_0$ &  \\
			$\vec{g_{0}}$ & gradient of matched Keyline from previous frame \\
			\bottomrule
		\end{tabularx}
		
		\begin{tablenotes}
			\footnotesize
			\item[a] Jakiś komentarz\textellipsis
		\end{tablenotes}
		
	\end{threeparttable}
\end{table}

% ---

\section{Edge extraction}

Primal step of algorithm is subpixel edge extraction. Keyline structures are populated using extracted data. Keylines that are estimated to lie on same edge are joined.

\subsection{Edge detection algorithm choice}
While many edge detection algorithms could be used in this step, authors of \textit{Rebvo} have chosen DoG (Difference of Gaussians), because it provides \cite{jose2015realtime}:
\begin{enumerate}
	\item repetivity -- an edge is detected similarly throughout consecutive frames,
	\item precision -- edge positions are accurate,
	\item low time \& memory complexity.
\end{enumerate}

Another advantage of DoG, unmentioned by Tarrio and Pedre, is that \underline{edge gradient can be obtained directly} from normal vector of the fitted local plane. In this thesis DoG was used as well. Most vital property was subpixel precision, otherwise Canny detector \cite{canny} would be employed. In principle, subpixel edge detection precision is possible, because as long as Whittaker–Nyquist–Kotelnikov–Shannon theorem assumptions are satisfied, the true continuous pixel intensity function can be reconstructed from discrete image values.

\subsection{Data preprocessing}

First of all, images are converted to grayscale. Color information is not used in the algorithm.

Before performing any edge detection technique, one critical issue has to be addressed. If input images were\footnote{If images were \textit{not} rectified, they should be.} rectified in such a way that had needed extrapolation beyond original borders, artificial edges will be present in every frame (because extrapolation procedure usually assumes 0 pixel intensity beyond the image). Gradient of these edges is almost always strong, meaning that they tend to pass all tests and to be identified as valid keylines, thus generating false positives. During later minimization step, they greatly distort the procedure, making it behave as if the space was curved. Example of such border is depicted in Fig.~\ref{fig:rectifcy_border}.

\begin{figure}[ht]
	\centering\includegraphics[width=0.75\linewidth, trim={1cm 1cm 1cm 1cm},clip]{img/figures/rectified_edge.png}
	\caption{ Input image, zoomed-in near left border. Individual pixels produced by rectification can be observed on the left }
	\label{fig:rectifcy_border}
\end{figure}

In case of DoG, these \underline{artificial edges need to be removed} before Gaussian blurring -- otherwise they would spread out, making it tricky to reject them later. In tested datasets, artificial borders' thickness did not exceed 1~pixel, so simply 1-pixel wide frame of outermost pixels was always discarded. In general case, \underline{width of the frame can be figured out from distortion model}, instead of being manually set. However, for highly distorted images a rectangular frame would also discard many valid pixels -- either in corners (barrel distortions), or near middle of borders (pincushion distortions).

\subsection{Difference of Gaussians strength tests}

After necessary preprocessing, edge detection can be started. Generally edge detection works by finding positions in image where pixel intensity changes most rapidly. Basic idea of DoG is to approximate image second derivative (the laplacian) \cite{szeliski}. The zero-crossing of laplacian corresponds to such positions. Other approaches to edge detection involve curve fitting \cite{fabijanska} \cite{devernay1995non}.

In order to filter out noise, image should be first smoothed with a Gaussian filter. These operations can be combined into one operator -- Laplacian of Gaussian -- which in turn can be approximated with difference between two images smoothed with Gaussian filters using two different sigmas\footnote{LoG is best approximated by DoG when $\frac{\sigma_{1}}{\sigma_{2}} = \sqrt{2}$ \cite{sift}.} Edge detection results for initial sigma values were satisfactory in performed tests, therefore other sigma values were not tested.

Exemplary Difference of Gaussians image is depicted in Fig.~\ref{fig:dog}. \textit{Implementation insight}:~while individual smoothed images can be computed using {\tt uint8} as underlying data type (for speed), the difference has to be computed using signed data type. Otherwise obtained function will not cross zero!

\begin{figure}[ht]
	\centering\includegraphics[width=0.75\linewidth, trim={1.25cm 1.25cm 1.25cm 1cm},clip]{img/figures/dog.png}
	\caption{ Difference of Gaussians applied to an image. Near-zero values indicate feasible edge candidates }
	\label{fig:dog}
\end{figure}

Another parameter is window size -- it defines pixel neighborhood that will take part in later calculations of edge position. For window size $w$, $(2w+1)^2$ immediate neighbors will be considered, including the center pixel itself. For sake of this thesis, value $w = 2$ was used.

\subsubsection{First test}
\label{edge_first}

Edge detection procedure is performed for every pixel that lies at least $w$ pixels from image border, so that neighborhood does not need to be extrapolated. In order to speed up computations by early identifying non-edges, first derivative of pixel intensity is calculated by applying two Sobel operators (derivatives along $x$ and $y$ axes) and taking norm of the results. This norm is then compared with a threshold. 

Tarrio and Pedre use hysteresis to determine threshold value after every frame. On the one hand, presented algorithm operates offline, so abundance of keylines is not an issue. On the other, hysteresis parameters still need to be adjusted for given dataset. Invalid parameters can alter overall algorithm results significantly, so from algorithm analysis point of view, the fewer of them, the better. \underline{Thus instead simpler solution was tested out}.

Image is be partitioned into $a^2$ rectangular chunks, for relatively small $a$, e.g. 7. Then for each chunk number of pixels that would pass the first derivative test for given constant threshold is counted. If this number is relatively low, threshold is locally multiplied by a constant $b_{1} < 1$, e.g. $\frac{1}{3}$. However, if the number is relatively large, then threshold is locally multiplied by $b_{2} > 1$, e.g. $\frac{5}{3}$. Local threshold array can finally be smoothed with Gaussian filter to avoid discontinuities.

Proposed procedure also has parameters, but there are few advantages:
\begin{itemize}
	\item parameters are much simpler to interpret,
	\item effectively only one parameter is crucial (the constant threshold) and rest of procedure serves as a refinement,
	\item operates without delay, which is vital especially if keyline number is too low.
\end{itemize}

Fig.~\ref{fig:bucket} illustrates that additional pixels were considered further for being edges. However, edges added by this method are not very stable and tend to be nonetheless untraceable in later frames (they do not exhibit the \textit{repetivity} property).

TODO: figure

\subsubsection{Second test}
\label{edge_second}

Neighborhood of pixels that have passed the first test is checked for ``sign balance``. Number of positive and negative DoG values has to be comparable within a defined percentage, e.g. 20\%. It was observed that pixels very rarely fail this particular test.

TODO: figure

\subsubsection{Third test}
\label{edge_third}

Then DoG values are approximated by a plane using linear regression (Eq.~\ref{eq:regress} is solved for~$\theta$). Zero-crossing of the plane can be determined analytically using Eq.~\ref{eq:zerocross}. For more resilience, an additional test is performed. If inequality~\ref{eq:test3} is not satisfied, then equation system~\ref{eq:regress} \underline{is considered to be badly conditioned} and this pixel is rejected. This was not considered in \cite{jose2015realtime}. The purpose of this test is to avoid divide-by-zero errors in fourth test (Sec.~\ref{edge_fourth}). All pixels that do not pass the third test would still be filtered out later -- they also do not pass the fifth test~(Sec.~\ref{edge_fith}).

\begin{equation}
\bm{A}\theta = \delta
\label{eq:regress}
\end{equation}
where:
\begin{eqwhere}[2cm]
	\item[$\bm{A}$] 3x$(2w+1)$ matrix: $[X\ Y\ 1]$,
	\item[$X$] vector of $x$ coordinates of pixel centroids in neighborhood, assuming that central pixel's centroid is located at $x = 0$, ($(2w+1)$-tuple),
	\item[$Y$] vector of $y$ coordinates of pixel centroids in neighborhood, assuming that central pixel's centroid is located at $y = 0$, ($(2w+1)$-tuple),
	\item[$\delta$] DoG values corresponding to $X$ and $Y$ ($(2w+1)$-tuple),
	\item[$\theta$] parameters defining the approximated plane: $z = \theta_{x}x + \theta_{y}y + \theta_{z}$.
\end{eqwhere}



\begin{equation}
\begin{bmatrix}
x_s \\
y_s 
\end{bmatrix} = 
\begin{bmatrix}
\frac{-\theta_{x} \theta{y}}{\theta_x^2 + \theta_y^2} \\
\frac{-\theta_{y} \theta{y}}{\theta_x^2 + \theta_y^2}
\end{bmatrix}
\label{eq:zerocross}
\end{equation}
where:
\begin{eqwhere}[2cm]
	\item[$k_s$] estimated $k$-coordinate of the subpixel edge position (0 is the pixel center).
\end{eqwhere}

\begin{equation}
\theta_{x}^2 + \theta_{y}^2 > 10^{-6}
\label{eq:test3}
\end{equation}

\subsubsection{Fourth test}
\label{edge_fourth}

Obtained zero-crossing marks the subpixel position of the edge. Inequality~\ref{eq:subpix_inside} tests whether it lies within the pixel itself. If not -- edge is not detected. After this test, most of Keyline candidates form 1-pixel wide edges.

\begin{equation}
max(|x_s|, |y_s|) < 0.5
\label{eq:subpix_inside}
\end{equation}

\subsubsection{Fifth test}
\label{edge_fith}

Normal vector of fitted plane is defined by Eq.~\ref{eq:planenorm}. Vector $\vec{r}$ can projected onto $z=0$ plane, creating third derivative vector: the edge gradient $\vec{g}$. If $|\vec{g}|$ is small, then $r_{z}$ component must have dominated $\vec{r}$, meaning that fitted plane was almost parallel to the $z=0$ plane. In turn this implies that edge was not sharp. Therefore as final test, norm of $\vec{g}$ is tested -- it must exceed a~threshold for edge to be finally detected.

\begin{equation}
\vec{r} = [\theta_{x},\ \ \theta_{y},\ \ -1]
\label{eq:planenorm}
\end{equation}
where:
\begin{eqwhere}[2cm]
	\item[$\vec{r}$] normal vector of plane $z = \theta_{x}x + \theta_{y}y + \theta_{z}$.
\end{eqwhere}

\subsubsection{Edge detection tests summary}

image: quiver, @jan

\begin{figure}[ht]
	\centering\includegraphics[width=1.0\linewidth, trim={1.5cm 2.5cm 1.5cm 2.5cm},clip]{img/figures/edge_prob.png}
	\caption{ Edge detection tests results. Pixel colors: dark blue -- neighborhood outside image; blue, light blue (not present), cyan (not present), green and orange -- rejected by tests 1 to 5, respectively; red -- reserved for debug purposes (not present); brown -- final Keylines}
	\label{fig:edgeprob}
\end{figure}


\section{Keyline initialization}

After Keyline has been successfully identified, a data structure described in Section~\ref{sec:struct} is populated with obtained data. This step is crucial for the very first processed frame, because its Keylines must have some initial depth values. During first tests, 

todo: prove that noise for rho is better than ones

imgmask with KL indices

\subsection{Keyline joining}

After obtaining individual Keylines, they are joined together to form connected edges.
For each Keyline, its neighbors are searched among 3 out of 8 bordering pixels. Search is performed in direction perpendicular to $\vec{g}$, an example is depicted in Fig.~\ref{fig:edgejoin}.

\begin{figure}[ht]
	\centering\includegraphics[width=0.75\linewidth, trim={1.25cm 1.25cm 1.25cm 1cm},clip]{img/edgejoin.png}
	\caption{ Edge joining principle. White pixels denote Keylines, black pixels \==~non-Keylines, green x -- currently processed pixel, green arrow -- $\vec{g}$, red arrow -- $\vec{g}$ rotated by $\frac{\pi}{2}$ clockwise , red rectangle -- edge joining candidates }
	\label{fig:edgejoin}
\end{figure}


Joined Keylines are used for pruning. First of all, edges consisting of 3 pixels or less are discarded, as they are unlikely to be good features to track. Secondly, outermost Keylines of every joined edge are likewise removed -- they are most likely to be affected by noise.

In later steps, the algorithm considers only individual Keylines, not joined edges (as mentioned in Section~\ref{sec:rebvo_outline}). Information about neighbors is used only once after edge detection -- in Regularization step, when $\rho$ and $\sigma_{\rho}$
are averaged over Keyline and its 2 immediate neighbors.

Initially \underline{more edge joining strategies were considered}: morphological operations on Keylines, loop avoidance and edge segmentation (into parts with similar gradient). However, once it has been understood that edge joining takes such small part in algorithm flow, this direction was abandoned.




%--

\section{Edge tracking}


minimizer

auxiliary image: a lookup table for minimizer

estimate quantile - its purpose is to cutoff KLs that have too high uncertainty with respect to all KLs

project KL\_prev to 3D using previously estimated rhos

one iteration of LM:

    using jacobian, new state vector is calculated for next iteration

    apply transformation given in argument

    project previous points from these 3D positions to 2D

    for every previous KL perfoms test to check if it should take part in minimization:

       test1: uncertainty below previously estimated quantile

	 test2: (excluding minimalization for very first frame) check if KL has appeared before

     estimate rewieghting

	 test3: check if projected KL lies within image frame

       test4: check if there is a KL in this pixel using auxiliary image

       test5: compare their gradients (escobar)

       we get residual (DResidual) \& weighted residual (fm) projected in direction of gradient

     calculate jacobians using fishy equations

     if there was gain, use these new parameters, otherwise ...

double init(zeros and priors): 3 iterations with no reweighting

use better result, proceed for 15 iterations with reweighting

transformation that resulted in lowest score is considered the optimal transformation between frames

estimate uncertainty as inverse of final jacobian

\subsection{Warping function}

\subsection{Auxiliary image}

todo: a lookup table for minimizer

\subsection{Keyline matching criteria}

todo Jan: figure

\subsection{Energy minimization}

\subsection{Initial conditions}

\subsection{Reweighing}

%--

\section{Mapping}

forward rotate

KL pixel positions, represented in normalized homo coords, are rotated using obtained rotation

if they don't go to infinity, they are projected back onto image plane

nasty gradient rotation

\subsection{Forward matching}

forward match

using information about which KLs were matched to which, following fields are propagated: rho, s\_rho, gradient, history and index of the matched KL

todo: could new edges apprer without directed match

\subsection{Directed matching}

directed match

 for each KL:

   backrotate KL and cast it onto old edge map mask

   using this position and backrotated velocity, obtain halfline in which this KL could have been moved (a halfline, because we only know that its rho is positive) - estimated pixel displacement

   estimate uncertainty using projected position and Rvel

   caonstrain displacement search radius \& define starting point

 todo: check if perpedicular displacement if really common


 search from starting point alternating between one direction and the other

 check if this pixel there is a KL to match to

 perform 3 tests to filter outliers + image (eg. 3 consecutive images where an outlier disappears):

  gradient angle similarity

  gradient modulus similarity

  motion consistency

 if there aren't enough matched KLs (500), reset

\subsection{Regularization}

 optional regularization, performed twice

 main assumption is that KLs neighboring on an edge ale located near each other in 3D, so their depth should be similiar

 image: show that this isn't ALWAYS true

 for each KL:

  check if 2 neighbors pass test:

   if depths outweight uncertainties (probabilistic uncertainty)

   if angle between gradients is below threshold

  if tests are passed, rho and s\_rho are smoothened taking weighted mean of its value and its neighbors

\subsection{Depth estimation}

 kalman fiter

 ...

 inverse depth is constrained between certain min and max

\subsection{Scale correction}


 optional estimate rescaling

 according to Tarrio et al, EKF is biased in rho estimation, so a global "shrinking factor" can be applied to depths and uncertainties.


% ----------------------







